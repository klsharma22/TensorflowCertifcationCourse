{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundatmentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language\n",
    "\n",
    "Another common term for NLP problem is seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading helper function inside the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going ti be using is Kaggle's introduction to NLP dataset. A classification problem\n",
    "\n",
    "[Competition Link](https://www.kaggle.com/competitions/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 11:10:55--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4009:805::201b, 2404:6800:4009:806::201b, 2404:6800:4009:809::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4009:805::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: 'nlp_getting_started.zip.1'\n",
      "\n",
      "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-03-12 11:10:55 (4.80 MB/s) - 'nlp_getting_started.zip.1' saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "\n",
    "unzip_data('nlp_getting_started.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read thed data\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac= 1, random_state= 42)\n",
    "\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class\n",
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How may samples\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Not a disaster\n",
      "Text: Mike Magner Discusses A Trust Betrayed: http://t.co/GETBjip5Rh via @YouTube #military #veterans #environment\n",
      "----\n",
      "\n",
      "Target: Not a disaster\n",
      "Text: Beware of your temper and a loose tongue! These two dangerous weapons combined can lead a person to the Hellfire #islam!\n",
      "----\n",
      "\n",
      "Target: Disaster\n",
      "Text: Flood: Two people dead 60 houses destroyed in Kaduna: Two people have been reportedly killed and 60 houses ut... http://t.co/BDsgF1CfaX\n",
      "----\n",
      "\n",
      "Target: Disaster\n",
      "Text: MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ... - ABC Onlin... http://t.co/N3lNdJKYo3 G #Malaysia #News\n",
      "----\n",
      "\n",
      "Target: Not a disaster\n",
      "Text: Whenever I have a meltdown and need someone @Becca_Caitlyn99 is always like 'leaving in 5' and I don't know how I got so lucky #blessed\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualise some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[['text', 'target']][random_index: random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {'Disaster' if target == 1 else 'Not a disaster'}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"----\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split() to split the trianing data into train and validation dataset\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(), train_df_shuffled['target'].to_numpy(), test_size= 0.1, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 sentences\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to nuumbers.\n",
    "\n",
    "There are a few ways to do this:\n",
    "* Tokenization\n",
    "* Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Use defalut TextVectorization parameter\n",
    "text_vectorizer = TextVectorization(max_tokens= None, # how many words in vocab\n",
    "                                    standardize= 'lower_and_strip_punctuation',\n",
    "                                    split= 'whitespace',\n",
    "                                    ngrams= None, # creates a group of words\n",
    "                                    output_mode= 'int', # in which format the output should be\n",
    "                                    output_sequence_length= None, # how long deos the output sequence should be of\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokens in the training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences]) // len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocab\n",
    "max_length = 15 # max length our sequences will be\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens= max_vocab_length,\n",
    "                                    output_mode= 'int',\n",
    "                                    output_sequence_length= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorization to training set\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentences = \"There's a flood in my street!\"\n",
    "\n",
    "text_vectorizer([sample_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: -??-\n",
      "; kitana\n",
      "? her fatalities slay me\n",
      "ÛÓkody? (Vine by @KOMBATFANS33) https://t.co/uMajwSNLUF\n",
      "Vectorized sentence: [[   1   81  301 8417   31 6431 1927   18    1    1    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sentences from the training sentences\n",
    "random_sentences = random.choice(train_sentences)\n",
    "print(f\"Original text: {random_sentences}\")\n",
    "print(f\"Vectorized sentence: {text_vectorizer([random_sentences])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo words in vocab: 10000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary() # retrieves all of the unique words\n",
    "\n",
    "tope_5_words = words_in_vocab[:5] # gets the most common words\n",
    "bottom_5_words = words_in_vocab[-5:] # gets the least common words\n",
    "\n",
    "print(f\"Number fo words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {tope_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding layer\n",
    "\n",
    "We're going to use Tensorflow Embedding layer.\n",
    "\n",
    "\n",
    "The parameters we care most about our embedding layers:\n",
    "* `input_dim`\n",
    "* `output_dim`\n",
    "* `input_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x2942dccd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim= max_vocab_length, # set input shape\n",
    "                             output_dim= 128,\n",
    "                             input_length= max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.043427  , -0.02271678, -0.00248923, ..., -0.03483094,\n",
       "         -0.01346254, -0.03058391],\n",
       "        [ 0.03267677,  0.04190341, -0.04285845, ...,  0.0135429 ,\n",
       "         -0.02445282, -0.03918562],\n",
       "        [ 0.01630927, -0.03321409, -0.00125452, ...,  0.02345994,\n",
       "          0.02224901,  0.00741848],\n",
       "        ...,\n",
       "        [-0.02616034,  0.00962585,  0.0474998 , ...,  0.03219023,\n",
       "          0.0456875 ,  0.04080323],\n",
       "        [-0.01575265,  0.04413174,  0.01501777, ..., -0.04161366,\n",
       "         -0.03297486,  0.00340591],\n",
       "        [-0.01612275,  0.04820825,  0.0210881 , ...,  0.02680815,\n",
       "          0.03131999, -0.00614514]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a ranomd sentence from the training set\n",
    "random_sentences = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original sentences: {random_sentences}\")\n",
    "\n",
    "# Embed the random sentences\n",
    "sample_embde = embedding(text_vectorizer([random_sentences]))\n",
    "sample_embde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.043427  , -0.02271678, -0.00248923, -0.04603863,  0.00509753,\n",
       "         0.01818783, -0.00246652,  0.01118038,  0.00157026,  0.04659058,\n",
       "        -0.01123718, -0.00972148,  0.04788823,  0.01407531, -0.02020611,\n",
       "        -0.03455592,  0.04628216, -0.04112779,  0.04135301, -0.02473733,\n",
       "         0.03040474,  0.02041018,  0.00029361, -0.04249674,  0.0377947 ,\n",
       "        -0.00522221,  0.03419198, -0.01971959,  0.02616808,  0.03351443,\n",
       "        -0.0458042 , -0.02779064,  0.04195997, -0.02212015, -0.03683157,\n",
       "         0.00789969,  0.02253792, -0.04799919, -0.03994947, -0.04821298,\n",
       "        -0.04259375,  0.02873099,  0.03656069, -0.04618149, -0.04235861,\n",
       "        -0.03062811,  0.02972574, -0.01694834,  0.03712305,  0.03289719,\n",
       "         0.04936178, -0.01675064, -0.04377675,  0.02584152,  0.04081562,\n",
       "        -0.00031313, -0.00864778, -0.00172927,  0.02708354, -0.0185649 ,\n",
       "        -0.04001318,  0.00172317,  0.01937211,  0.00556774,  0.00397693,\n",
       "        -0.03631838, -0.04462849, -0.02080899,  0.04514698, -0.01922944,\n",
       "        -0.02374872, -0.0468714 , -0.01077871, -0.03157955, -0.04873264,\n",
       "        -0.00965784, -0.02315221,  0.03655436, -0.04466926,  0.01839526,\n",
       "        -0.04151868, -0.01684328,  0.00872016, -0.03104689,  0.02647151,\n",
       "        -0.0468613 ,  0.0338442 , -0.0098135 , -0.01127733,  0.04500835,\n",
       "         0.03906084, -0.04139799, -0.04829485,  0.04400598, -0.0224173 ,\n",
       "         0.01750404, -0.00151638,  0.01769127,  0.02168074, -0.02500168,\n",
       "        -0.04846506,  0.01938829, -0.01911032, -0.04346162, -0.04515893,\n",
       "        -0.02620918, -0.048883  ,  0.04580424,  0.02927433, -0.02087658,\n",
       "         0.0016773 , -0.0059906 , -0.02303615, -0.03287779,  0.02705491,\n",
       "        -0.03646349,  0.04304031,  0.02123678, -0.00470868, -0.02931422,\n",
       "         0.0488063 , -0.01934584,  0.00137708, -0.0493651 , -0.0026213 ,\n",
       "        -0.03483094, -0.01346254, -0.03058391], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 't')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embde[0][0], sample_embde[0][0].shape, random_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling a text dataset\n",
    "\n",
    "Experiments:\n",
    "\n",
    "* Model 0: Naive Bayes\n",
    "* Model 1: Feed-forward neural network\n",
    "* Model 2: LSTM model\n",
    "* Model 3: GRU model\n",
    "* Model 4: Bidirectional LSTM model\n",
    "* Model 5: 1D Convolutional Neural Network\n",
    "* Model 6: Tensorflow Hub pretrained fetaure extractor\n",
    "* Model 7: Same as Model 6 but with 10% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "> **Note**: It is common practice to use non-DL algorithm as a baseline because of their speed and then alter using DL to see if you can improve upon them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenizer and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert the text data into numerical form using tf-idf technique\n",
    "    (\"clf\", MultinomialNB()) # classifier for the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves: {baseline_score * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function to evaluate the model\n",
    "# A function which returns a dictionary of accuracy, precision, recall and f1-score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    '''\n",
    "    This function gives the evaluation of the classification model in a dictionary form (Accuracy, Precision, Recall and F1-score)\n",
    "\n",
    "    Parameters:\n",
    "        y_true: The true labels of the evaluation data\n",
    "        y_pred: The predicted labels of the evaluation data\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys: `accuracy`, `precision`, `recall`, `f1-score`\n",
    "    '''\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1score = f1_score(y_true, y_pred, average= 'weighted') # we are using weighted  average because the dataset is slightly imbalanced\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = evaluation(y_true= val_labels, \n",
    "           y_pred= baseline_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save tensorboard callback logs\n",
    "SAVED_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functiobnal API\n",
    "\n",
    "inputs = layers.Input(shape= (1, ), dtype= tf.string)\n",
    "x = text_vectorizer(inputs) # convert text to numbers\n",
    "x = embedding(x) # convert numbers to matrix\n",
    "x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name= 'model_1_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glo  (None, 128)               0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_1.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20240312-130658\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 16ms/step - loss: 0.6111 - accuracy: 0.6871 - val_loss: 0.5362 - val_accuracy: 0.7585\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.4403 - accuracy: 0.8183 - val_loss: 0.4701 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.3456 - accuracy: 0.8610 - val_loss: 0.4577 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2838 - accuracy: 0.8920 - val_loss: 0.4646 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2376 - accuracy: 0.9099 - val_loss: 0.4869 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(x = train_sentences,\n",
    "                              y= train_labels,\n",
    "                              epochs= 5,\n",
    "                              validation_data= (val_sentences, val_labels),\n",
    "                              callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_1_dense')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4869 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4868801534175873, 0.7887139320373535]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = model_1.predict(val_sentences)\n",
    "model_1_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = evaluation(y_true= val_labels,\n",
    "           y_pred= tf.round(tf.squeeze(model_1_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glo  (None, 128)               0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got the embedding matrix, let's see how to visualize it. To do so, Tensorflow has a handy tool called [projector](https://projector.tensorflow.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the weight matrix of the embedding layer\n",
    "embed_weights = model_1.get_layer('embedding').get_weights()\n",
    "embed_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding files\n",
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[0][index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "\n",
    "LSTM = Long Short Term Memory\n",
    "\n",
    "Architecture of the model:\n",
    "\n",
    "```\n",
    "Input(text) -> Tokenize -> Embedding -> Layers -> Output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LSTM Model\n",
    "inputs = layers.Input(shape = (1, ), dtype= \"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.LSTM(units= 64, return_sequences= True, name= 'LSTM_layer_1')(x) # when stacking RNN cells together you need to set return_sequences = True \n",
    "# x = layers.LSTM(64, name= 'LSTM_layer_2')(x)\n",
    "x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "x = layers.Dense(64, activation= 'relu', name= 'dense_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs, name= 'model_2_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " LSTM_layer_1 (LSTM)         (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glo  (None, 64)                0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " dense_layer (Dense)         (None, 64)                4160      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1333633 (5.09 MB)\n",
      "Trainable params: 1333633 (5.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20240312-154939\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 26ms/step - loss: 0.1251 - accuracy: 0.9545 - val_loss: 1.2042 - val_accuracy: 0.7585\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0629 - accuracy: 0.9714 - val_loss: 1.4600 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0465 - accuracy: 0.9775 - val_loss: 1.6172 - val_accuracy: 0.7520\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0409 - accuracy: 0.9778 - val_loss: 1.9617 - val_accuracy: 0.7480\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0424 - accuracy: 0.9764 - val_loss: 1.8052 - val_accuracy: 0.7559\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(x = train_sentences,\n",
    "                              y = train_labels,\n",
    "                              epochs= 5,\n",
    "                              validation_data = (val_sentences, val_labels),\n",
    "                              callbacks = [create_tensorboard_callback(SAVED_DIR, 'model_2_LSTM')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.2121449e-02],\n",
       "       [3.7462115e-01],\n",
       "       [9.9999964e-01],\n",
       "       [1.4338018e-01],\n",
       "       [3.1619800e-07],\n",
       "       [9.9999928e-01],\n",
       "       [9.9982184e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [9.9984944e-01]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred = model_2.predict(val_sentences)\n",
    "model_2_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7559055118110236,\n",
       " 'precision': 0.7755102040816326,\n",
       " 'recall': 0.6551724137931034,\n",
       " 'f1-score': 0.7531121360163758}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_results = evaluation(y_true= val_labels,\n",
    "                             y_pred= tf.squeeze(tf.round(model_2_pred)))\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_2_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effective RNN component is the GRU or gated recurrent unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an RNN using the GRU cell\n",
    "inputs = layers.Input(shape= (1, ), dtype= 'string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(64, name= 'GRU_layer_1')(x) # if you want to stack GRU cell return_sequences = True\n",
    "# x = layers.LSTM(42, name= 'LSTM_layer_1', return_sequences= True)(x)\n",
    "# x = layers.GRU(99, name= 'GRU_layer_2')(x)\n",
    "# x = layers.Dense(64, activation= 'relu', name= 'dense_layer_1')(x)\n",
    "# x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name= 'model_3_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " GRU_layer_1 (GRU)           (None, 64)                37248     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1317313 (5.03 MB)\n",
      "Trainable params: 1317313 (5.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20240312-161911\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 23ms/step - loss: 0.1432 - accuracy: 0.9438 - val_loss: 0.8526 - val_accuracy: 0.7638\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0619 - accuracy: 0.9765 - val_loss: 0.9589 - val_accuracy: 0.7703\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0485 - accuracy: 0.9778 - val_loss: 1.1092 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.0389 - accuracy: 0.9803 - val_loss: 1.3824 - val_accuracy: 0.7638\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.0403 - accuracy: 0.9813 - val_loss: 1.5092 - val_accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(x= train_sentences,\n",
    "                                y= train_labels,\n",
    "                                epochs= 5,\n",
    "                                validation_data= (val_sentences, val_labels),\n",
    "                                callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_3_GRU')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.45957386e-03],\n",
       "       [7.60579944e-01],\n",
       "       [9.99890208e-01],\n",
       "       [1.72754258e-01],\n",
       "       [1.05260398e-04],\n",
       "       [9.99799073e-01],\n",
       "       [9.97497857e-01],\n",
       "       [9.99913931e-01],\n",
       "       [9.99870777e-01],\n",
       "       [9.99759734e-01],\n",
       "       [4.68147569e-04],\n",
       "       [9.99819338e-01],\n",
       "       [6.29706716e-04],\n",
       "       [1.40619487e-01],\n",
       "       [1.07793210e-04],\n",
       "       [1.38186326e-03],\n",
       "       [1.95411063e-04],\n",
       "       [2.74125428e-04],\n",
       "       [7.56117776e-02],\n",
       "       [9.99833584e-01],\n",
       "       [9.99946713e-01],\n",
       "       [1.16025723e-04],\n",
       "       [9.99264777e-01],\n",
       "       [3.48688685e-04],\n",
       "       [9.99891877e-01],\n",
       "       [9.99878645e-01],\n",
       "       [3.58359597e-04],\n",
       "       [4.92727559e-04],\n",
       "       [7.12124573e-04],\n",
       "       [9.93301868e-01],\n",
       "       [9.99805987e-01],\n",
       "       [3.42555752e-04],\n",
       "       [5.10615390e-03],\n",
       "       [1.87861151e-03],\n",
       "       [9.59347904e-01],\n",
       "       [3.84412438e-01],\n",
       "       [9.99834538e-01],\n",
       "       [1.98423192e-01],\n",
       "       [9.77922510e-03],\n",
       "       [9.99879837e-01],\n",
       "       [9.99041975e-01],\n",
       "       [1.01081641e-04],\n",
       "       [9.99683380e-01],\n",
       "       [1.79275667e-04],\n",
       "       [3.63669783e-01],\n",
       "       [9.99836087e-01],\n",
       "       [9.99782741e-01],\n",
       "       [9.99212265e-01],\n",
       "       [2.28017312e-03],\n",
       "       [9.95552003e-01],\n",
       "       [9.14620236e-04],\n",
       "       [5.17422378e-01],\n",
       "       [3.25411707e-01],\n",
       "       [2.21388042e-02],\n",
       "       [4.43077356e-01],\n",
       "       [3.88171570e-03],\n",
       "       [1.06354267e-03],\n",
       "       [9.99854445e-01],\n",
       "       [4.81645373e-04],\n",
       "       [7.36026559e-04],\n",
       "       [2.07938179e-02],\n",
       "       [9.99850631e-01],\n",
       "       [9.97723758e-01],\n",
       "       [7.14832102e-04],\n",
       "       [9.99911904e-01],\n",
       "       [9.99900699e-01],\n",
       "       [9.96609330e-01],\n",
       "       [9.40711737e-01],\n",
       "       [9.99720752e-01],\n",
       "       [8.55465010e-02],\n",
       "       [5.12421504e-03],\n",
       "       [9.36344638e-03],\n",
       "       [9.99930382e-01],\n",
       "       [8.10064666e-04],\n",
       "       [9.99784291e-01],\n",
       "       [9.98713970e-01],\n",
       "       [1.84132752e-03],\n",
       "       [9.99762714e-01],\n",
       "       [4.39274997e-01],\n",
       "       [7.41699385e-03],\n",
       "       [5.19333582e-04],\n",
       "       [2.66295642e-01],\n",
       "       [9.99874473e-01],\n",
       "       [4.34982590e-04],\n",
       "       [5.96770842e-04],\n",
       "       [6.35514618e-04],\n",
       "       [3.33479955e-04],\n",
       "       [4.04425431e-04],\n",
       "       [4.09973087e-03],\n",
       "       [9.99855042e-01],\n",
       "       [9.99844909e-01],\n",
       "       [3.78903147e-04],\n",
       "       [9.98816967e-01],\n",
       "       [2.13249965e-04],\n",
       "       [9.99866962e-01],\n",
       "       [8.13990235e-01],\n",
       "       [9.99797165e-01],\n",
       "       [9.99925852e-01],\n",
       "       [9.99826133e-01],\n",
       "       [9.98200774e-01],\n",
       "       [9.99925137e-01],\n",
       "       [3.46690125e-04],\n",
       "       [1.39869284e-04],\n",
       "       [9.99775469e-01],\n",
       "       [9.99829769e-01],\n",
       "       [9.75352363e-04],\n",
       "       [9.97942269e-01],\n",
       "       [9.99217629e-01],\n",
       "       [2.64852279e-04],\n",
       "       [9.99805391e-01],\n",
       "       [9.99426484e-01],\n",
       "       [2.79319298e-04],\n",
       "       [9.99664664e-01],\n",
       "       [4.77101741e-04],\n",
       "       [2.29977886e-04],\n",
       "       [3.36985383e-03],\n",
       "       [9.81124878e-01],\n",
       "       [9.99851704e-01],\n",
       "       [4.91087150e-04],\n",
       "       [1.59538933e-04],\n",
       "       [9.99896407e-01],\n",
       "       [1.21988094e-04],\n",
       "       [2.75449507e-04],\n",
       "       [9.93498206e-01],\n",
       "       [2.13306062e-02],\n",
       "       [5.86259761e-04],\n",
       "       [9.99808371e-01],\n",
       "       [1.31837922e-04],\n",
       "       [4.40130447e-04],\n",
       "       [9.99809682e-01],\n",
       "       [1.29086978e-03],\n",
       "       [9.99896407e-01],\n",
       "       [9.99904513e-01],\n",
       "       [9.99878645e-01],\n",
       "       [9.99833703e-01],\n",
       "       [3.43453052e-04],\n",
       "       [9.99843001e-01],\n",
       "       [7.76478171e-01],\n",
       "       [1.20458077e-03],\n",
       "       [3.13400058e-04],\n",
       "       [9.99906182e-01],\n",
       "       [9.94556308e-01],\n",
       "       [1.40619487e-01],\n",
       "       [9.99678850e-01],\n",
       "       [1.58637613e-01],\n",
       "       [9.16874851e-04],\n",
       "       [2.98221916e-04],\n",
       "       [1.82579388e-04],\n",
       "       [9.94918168e-01],\n",
       "       [9.99847770e-01],\n",
       "       [8.67713068e-04],\n",
       "       [2.25234494e-01],\n",
       "       [1.25471817e-03],\n",
       "       [1.94947308e-04],\n",
       "       [7.58926384e-04],\n",
       "       [9.99790490e-01],\n",
       "       [9.87278700e-01],\n",
       "       [7.59273674e-03],\n",
       "       [9.99741495e-01],\n",
       "       [1.79588067e-04],\n",
       "       [9.99866605e-01],\n",
       "       [6.83131767e-03],\n",
       "       [1.88925751e-02],\n",
       "       [9.94001210e-01],\n",
       "       [2.39965953e-02],\n",
       "       [1.56846945e-04],\n",
       "       [9.99869704e-01],\n",
       "       [7.41285388e-04],\n",
       "       [9.99892235e-01],\n",
       "       [9.99044359e-01],\n",
       "       [9.99883652e-01],\n",
       "       [9.99772251e-01],\n",
       "       [9.99832749e-01],\n",
       "       [2.85466784e-04],\n",
       "       [9.99932289e-01],\n",
       "       [2.25258220e-04],\n",
       "       [1.68621074e-02],\n",
       "       [1.01388833e-02],\n",
       "       [3.68211716e-01],\n",
       "       [9.99863982e-01],\n",
       "       [1.09005556e-03],\n",
       "       [9.99629617e-01],\n",
       "       [9.99849439e-01],\n",
       "       [9.99876738e-01],\n",
       "       [9.99875665e-01],\n",
       "       [3.98114586e-04],\n",
       "       [3.86054075e-04],\n",
       "       [9.99944568e-01],\n",
       "       [9.95203591e-05],\n",
       "       [6.27943373e-05],\n",
       "       [5.70952799e-03],\n",
       "       [9.99823511e-01],\n",
       "       [3.13135155e-04],\n",
       "       [2.78218446e-04],\n",
       "       [5.27105003e-04],\n",
       "       [3.20556900e-03],\n",
       "       [2.97917548e-04],\n",
       "       [6.27991976e-04],\n",
       "       [9.99840021e-01],\n",
       "       [3.83828813e-03],\n",
       "       [8.01446941e-03],\n",
       "       [9.99875307e-01],\n",
       "       [9.99854684e-01],\n",
       "       [9.99434650e-01],\n",
       "       [1.31643494e-03],\n",
       "       [9.99843717e-01],\n",
       "       [9.99735057e-01],\n",
       "       [9.99950767e-01],\n",
       "       [9.99757946e-01],\n",
       "       [9.96595919e-01],\n",
       "       [9.61512607e-03],\n",
       "       [9.99849558e-01],\n",
       "       [8.19176494e-04],\n",
       "       [5.31249854e-04],\n",
       "       [1.19007236e-04],\n",
       "       [1.02850157e-04],\n",
       "       [9.99878049e-01],\n",
       "       [9.82311726e-01],\n",
       "       [9.99857664e-01],\n",
       "       [8.86273503e-01],\n",
       "       [9.96385694e-01],\n",
       "       [1.02765625e-02],\n",
       "       [5.96368278e-04],\n",
       "       [1.67584023e-03],\n",
       "       [9.99843478e-01],\n",
       "       [2.90905703e-02],\n",
       "       [4.58514551e-04],\n",
       "       [9.99927521e-01],\n",
       "       [9.99809563e-01],\n",
       "       [9.99814212e-01],\n",
       "       [3.01971042e-04],\n",
       "       [6.36076136e-03],\n",
       "       [9.99812901e-01],\n",
       "       [7.35823750e-01],\n",
       "       [6.75265253e-01],\n",
       "       [1.44587411e-03],\n",
       "       [9.56396282e-01],\n",
       "       [2.02691741e-02],\n",
       "       [2.30034441e-03],\n",
       "       [1.10192702e-03],\n",
       "       [9.99799788e-01],\n",
       "       [2.58345623e-03],\n",
       "       [9.99855280e-01],\n",
       "       [9.99836564e-01],\n",
       "       [5.92911674e-04],\n",
       "       [2.00135750e-04],\n",
       "       [9.99834180e-01],\n",
       "       [1.91307234e-04],\n",
       "       [9.09615715e-04],\n",
       "       [3.97300005e-01],\n",
       "       [2.10586280e-04],\n",
       "       [9.99859810e-01],\n",
       "       [9.17587386e-05],\n",
       "       [4.60141629e-04],\n",
       "       [9.99839187e-01],\n",
       "       [9.99304131e-02],\n",
       "       [9.99865770e-01],\n",
       "       [9.99909520e-01],\n",
       "       [7.95352925e-03],\n",
       "       [2.49529490e-04],\n",
       "       [2.58203770e-04],\n",
       "       [1.62227638e-03],\n",
       "       [7.00838427e-05],\n",
       "       [9.99692440e-01],\n",
       "       [9.99832392e-01],\n",
       "       [2.06777632e-01],\n",
       "       [9.99825060e-01],\n",
       "       [4.23717691e-04],\n",
       "       [4.79378592e-04],\n",
       "       [1.91830436e-03],\n",
       "       [8.87488248e-04],\n",
       "       [5.30319929e-04],\n",
       "       [9.99836564e-01],\n",
       "       [7.98211026e-04],\n",
       "       [1.19667828e-04],\n",
       "       [9.99838471e-01],\n",
       "       [1.79250870e-04],\n",
       "       [2.30130769e-04],\n",
       "       [9.99841213e-01],\n",
       "       [3.23815607e-02],\n",
       "       [9.99113739e-01],\n",
       "       [2.45274976e-04],\n",
       "       [9.99836802e-01],\n",
       "       [9.90452886e-01],\n",
       "       [5.50408840e-01],\n",
       "       [7.95774609e-02],\n",
       "       [9.99823391e-01],\n",
       "       [2.48343014e-04],\n",
       "       [9.98724997e-01],\n",
       "       [5.19672073e-02],\n",
       "       [9.99811590e-01],\n",
       "       [9.98955011e-01],\n",
       "       [9.99580324e-01],\n",
       "       [9.97223616e-01],\n",
       "       [4.99184709e-04],\n",
       "       [8.59804511e-01],\n",
       "       [1.98515854e-03],\n",
       "       [6.10782939e-04],\n",
       "       [6.93014415e-04],\n",
       "       [3.60146344e-01],\n",
       "       [6.48650312e-05],\n",
       "       [4.82194300e-04],\n",
       "       [4.67430167e-02],\n",
       "       [9.99873519e-01],\n",
       "       [5.33249229e-04],\n",
       "       [3.72381136e-02],\n",
       "       [2.31965967e-02],\n",
       "       [2.26082906e-01],\n",
       "       [9.99614477e-01],\n",
       "       [1.81036026e-04],\n",
       "       [3.21003201e-04],\n",
       "       [9.99810636e-01],\n",
       "       [1.42224088e-01],\n",
       "       [4.33278918e-01],\n",
       "       [9.99922752e-01],\n",
       "       [5.35824220e-04],\n",
       "       [8.06991935e-01],\n",
       "       [1.26047567e-01],\n",
       "       [1.75893866e-03],\n",
       "       [1.53631121e-01],\n",
       "       [3.91590787e-04],\n",
       "       [3.28387413e-03],\n",
       "       [9.99837995e-01],\n",
       "       [5.41397720e-04],\n",
       "       [9.99843121e-01],\n",
       "       [2.02954546e-01],\n",
       "       [3.74530762e-04],\n",
       "       [9.99853253e-01],\n",
       "       [7.05607643e-04],\n",
       "       [9.99858022e-01],\n",
       "       [4.64622339e-04],\n",
       "       [5.30611956e-04],\n",
       "       [9.99855399e-01],\n",
       "       [3.90752306e-04],\n",
       "       [2.86920258e-04],\n",
       "       [9.99776542e-01],\n",
       "       [5.74180332e-04],\n",
       "       [4.14155598e-04],\n",
       "       [9.62267935e-01],\n",
       "       [9.99751389e-01],\n",
       "       [1.05351683e-04],\n",
       "       [3.05923610e-03],\n",
       "       [9.99838829e-01],\n",
       "       [9.99898911e-01],\n",
       "       [9.99779761e-01],\n",
       "       [7.09438547e-02],\n",
       "       [9.99603450e-01],\n",
       "       [9.99454796e-01],\n",
       "       [1.01739878e-03],\n",
       "       [1.11752865e-03],\n",
       "       [5.76469675e-03],\n",
       "       [3.75842955e-03],\n",
       "       [2.83421716e-04],\n",
       "       [4.89319628e-03],\n",
       "       [9.68361437e-01],\n",
       "       [4.50667925e-04],\n",
       "       [9.99843121e-01],\n",
       "       [9.99878645e-01],\n",
       "       [9.99895811e-01],\n",
       "       [1.74157787e-03],\n",
       "       [2.61103719e-01],\n",
       "       [1.09888439e-03],\n",
       "       [9.76950116e-03],\n",
       "       [9.99271572e-01],\n",
       "       [2.73722023e-01],\n",
       "       [1.72371743e-04],\n",
       "       [1.62116229e-03],\n",
       "       [8.25243013e-04],\n",
       "       [9.99745667e-01],\n",
       "       [2.24463688e-03],\n",
       "       [1.46628742e-03],\n",
       "       [3.42670246e-04],\n",
       "       [3.05044290e-04],\n",
       "       [1.36001289e-01],\n",
       "       [9.98758435e-01],\n",
       "       [7.90730566e-02],\n",
       "       [7.35446694e-04],\n",
       "       [2.11337511e-03],\n",
       "       [1.33742142e-04],\n",
       "       [9.99881983e-01],\n",
       "       [9.99831438e-01],\n",
       "       [9.99698162e-01],\n",
       "       [9.99813020e-01],\n",
       "       [1.41594312e-04],\n",
       "       [9.99558508e-01],\n",
       "       [9.99867916e-01],\n",
       "       [6.38461053e-01],\n",
       "       [8.62995721e-03],\n",
       "       [9.99788702e-01],\n",
       "       [5.65240718e-03],\n",
       "       [9.99896526e-01],\n",
       "       [9.98042583e-01],\n",
       "       [8.45304981e-04],\n",
       "       [3.69760871e-01],\n",
       "       [7.82410875e-02],\n",
       "       [9.99823153e-01],\n",
       "       [7.46282190e-03],\n",
       "       [9.10912786e-05],\n",
       "       [3.91270791e-04],\n",
       "       [2.73722023e-01],\n",
       "       [9.99943018e-01],\n",
       "       [1.17162192e-04],\n",
       "       [1.95191070e-01],\n",
       "       [9.99928474e-01],\n",
       "       [1.18152195e-04],\n",
       "       [9.99896407e-01],\n",
       "       [5.81743137e-04],\n",
       "       [3.95686104e-04],\n",
       "       [2.23896801e-04],\n",
       "       [9.99830723e-01],\n",
       "       [9.99752223e-01],\n",
       "       [7.73280277e-04],\n",
       "       [2.08105615e-04],\n",
       "       [9.99865890e-01],\n",
       "       [9.99864340e-01],\n",
       "       [6.84951425e-01],\n",
       "       [1.99001268e-04],\n",
       "       [2.11953837e-03],\n",
       "       [1.90796182e-01],\n",
       "       [1.90865656e-03],\n",
       "       [9.99906540e-01],\n",
       "       [9.92095292e-01],\n",
       "       [9.99900579e-01],\n",
       "       [9.99833822e-01],\n",
       "       [1.27333438e-03],\n",
       "       [5.51900640e-02],\n",
       "       [1.63123870e-04],\n",
       "       [9.99838710e-01],\n",
       "       [9.97305870e-01],\n",
       "       [9.99801695e-01],\n",
       "       [2.62416271e-03],\n",
       "       [7.11597875e-03],\n",
       "       [2.46887538e-03],\n",
       "       [1.92778520e-04],\n",
       "       [1.56760886e-01],\n",
       "       [7.03846628e-04],\n",
       "       [9.99675274e-01],\n",
       "       [8.24523682e-04],\n",
       "       [9.99895930e-01],\n",
       "       [9.99879360e-01],\n",
       "       [9.76968464e-03],\n",
       "       [7.60579944e-01],\n",
       "       [2.43529002e-03],\n",
       "       [2.43310104e-04],\n",
       "       [4.60286438e-01],\n",
       "       [9.99908209e-01],\n",
       "       [1.71825752e-01],\n",
       "       [1.04180306e-01],\n",
       "       [1.35075039e-04],\n",
       "       [1.12799241e-03],\n",
       "       [4.24991995e-05],\n",
       "       [9.99856591e-01],\n",
       "       [9.99346077e-01],\n",
       "       [9.99859095e-01],\n",
       "       [9.98851895e-01],\n",
       "       [9.99181688e-01],\n",
       "       [3.38696991e-03],\n",
       "       [1.21155987e-04],\n",
       "       [9.99928951e-01],\n",
       "       [9.99795616e-01],\n",
       "       [9.99862671e-01],\n",
       "       [9.90218716e-04],\n",
       "       [1.05620483e-02],\n",
       "       [6.58728532e-04],\n",
       "       [9.99866128e-01],\n",
       "       [9.99856830e-01],\n",
       "       [3.71391714e-01],\n",
       "       [9.94683206e-01],\n",
       "       [9.99925733e-01],\n",
       "       [1.37319779e-02],\n",
       "       [2.10216711e-03],\n",
       "       [9.99251187e-01],\n",
       "       [4.36970789e-04],\n",
       "       [2.47637217e-04],\n",
       "       [9.99791324e-01],\n",
       "       [2.65062749e-01],\n",
       "       [9.99452293e-01],\n",
       "       [9.99937177e-01],\n",
       "       [5.73249592e-04],\n",
       "       [1.38923747e-03],\n",
       "       [9.37163131e-05],\n",
       "       [1.28590167e-04],\n",
       "       [3.48239701e-04],\n",
       "       [9.99853730e-01],\n",
       "       [3.38802696e-04],\n",
       "       [8.59077096e-01],\n",
       "       [9.99709070e-01],\n",
       "       [1.16988020e-02],\n",
       "       [7.25662410e-02],\n",
       "       [1.46287683e-04],\n",
       "       [4.08670283e-04],\n",
       "       [9.99913454e-01],\n",
       "       [9.99850512e-01],\n",
       "       [1.74229441e-03],\n",
       "       [2.09090111e-04],\n",
       "       [4.17091476e-04],\n",
       "       [5.09542471e-04],\n",
       "       [9.99830246e-01],\n",
       "       [7.62341893e-04],\n",
       "       [9.99817789e-01],\n",
       "       [9.99641180e-01],\n",
       "       [9.98330295e-01],\n",
       "       [9.99381781e-01],\n",
       "       [3.58698033e-02],\n",
       "       [1.19150640e-03],\n",
       "       [4.22542304e-04],\n",
       "       [8.35331753e-02],\n",
       "       [9.42202747e-01],\n",
       "       [9.38196480e-01],\n",
       "       [1.74901918e-01],\n",
       "       [1.81394236e-04],\n",
       "       [5.60109984e-05],\n",
       "       [5.34350658e-03],\n",
       "       [1.32220052e-03],\n",
       "       [3.81365389e-04],\n",
       "       [1.81241199e-01],\n",
       "       [9.99893069e-01],\n",
       "       [9.99828339e-01],\n",
       "       [8.13990235e-01],\n",
       "       [9.99852061e-01],\n",
       "       [5.11742979e-02],\n",
       "       [8.87858623e-04],\n",
       "       [9.99819577e-01],\n",
       "       [1.91453646e-03],\n",
       "       [2.77231040e-04],\n",
       "       [1.32634476e-01],\n",
       "       [9.99012947e-01],\n",
       "       [1.28443717e-04],\n",
       "       [9.84649599e-01],\n",
       "       [9.99295473e-01],\n",
       "       [9.99756634e-01],\n",
       "       [9.99833822e-01],\n",
       "       [3.82970553e-04],\n",
       "       [5.04259253e-03],\n",
       "       [9.94693935e-01],\n",
       "       [1.95525310e-04],\n",
       "       [1.63223874e-03],\n",
       "       [6.25532912e-03],\n",
       "       [9.99757946e-01],\n",
       "       [9.97915566e-01],\n",
       "       [8.32905178e-04],\n",
       "       [2.13960142e-04],\n",
       "       [9.99642611e-01],\n",
       "       [2.73194513e-04],\n",
       "       [9.91165405e-04],\n",
       "       [2.12220388e-04],\n",
       "       [4.38147902e-01],\n",
       "       [9.99875426e-01],\n",
       "       [9.99790490e-01],\n",
       "       [8.02362442e-01],\n",
       "       [9.99895692e-01],\n",
       "       [9.99878168e-01],\n",
       "       [8.18415647e-05],\n",
       "       [9.99841809e-01],\n",
       "       [3.39029566e-03],\n",
       "       [9.99849319e-01],\n",
       "       [5.30353367e-01],\n",
       "       [7.57063506e-04],\n",
       "       [3.10134114e-04],\n",
       "       [1.83642158e-04],\n",
       "       [8.25827825e-04],\n",
       "       [7.42612610e-05],\n",
       "       [2.56971293e-03],\n",
       "       [2.13033956e-04],\n",
       "       [9.99856472e-01],\n",
       "       [3.14012868e-04],\n",
       "       [9.98524487e-01],\n",
       "       [9.99652982e-01],\n",
       "       [6.08978514e-03],\n",
       "       [1.73403881e-04],\n",
       "       [9.99903083e-01],\n",
       "       [5.09547826e-04],\n",
       "       [9.99854326e-01],\n",
       "       [2.81145722e-01],\n",
       "       [1.57171357e-02],\n",
       "       [2.22304799e-02],\n",
       "       [1.75169716e-03],\n",
       "       [2.60132784e-03],\n",
       "       [9.99878168e-01],\n",
       "       [1.87540994e-04],\n",
       "       [2.53338687e-04],\n",
       "       [1.73568636e-01],\n",
       "       [9.99865770e-01],\n",
       "       [2.40018219e-03],\n",
       "       [1.69232677e-04],\n",
       "       [9.99579728e-01],\n",
       "       [1.15897674e-04],\n",
       "       [5.05475851e-04],\n",
       "       [3.85123683e-04],\n",
       "       [6.26378179e-01],\n",
       "       [3.16897611e-04],\n",
       "       [3.32819343e-01],\n",
       "       [1.17283594e-03],\n",
       "       [2.24020076e-03],\n",
       "       [2.58787157e-04],\n",
       "       [1.53547945e-03],\n",
       "       [2.62875168e-04],\n",
       "       [9.99884963e-01],\n",
       "       [9.98049498e-01],\n",
       "       [1.16443932e-01],\n",
       "       [1.39542375e-04],\n",
       "       [9.72547743e-04],\n",
       "       [9.99889135e-01],\n",
       "       [9.98904347e-01],\n",
       "       [9.99917626e-01],\n",
       "       [9.20520572e-04],\n",
       "       [9.99798477e-01],\n",
       "       [3.38117359e-04],\n",
       "       [9.99842405e-01],\n",
       "       [9.52035129e-01],\n",
       "       [1.24999744e-04],\n",
       "       [9.99910116e-01],\n",
       "       [5.94642246e-04],\n",
       "       [9.99761164e-01],\n",
       "       [9.99906778e-01],\n",
       "       [4.01364872e-03],\n",
       "       [1.23473903e-04],\n",
       "       [9.92308140e-01],\n",
       "       [2.38230088e-04],\n",
       "       [9.99570549e-01],\n",
       "       [9.99843717e-01],\n",
       "       [3.44404876e-01],\n",
       "       [9.99836445e-01],\n",
       "       [1.75128758e-01],\n",
       "       [9.99839425e-01],\n",
       "       [9.99790370e-01],\n",
       "       [4.65944503e-03],\n",
       "       [1.42087301e-04],\n",
       "       [9.99548852e-01],\n",
       "       [1.53705812e-04],\n",
       "       [2.18663707e-01],\n",
       "       [9.99823987e-01],\n",
       "       [9.99810040e-01],\n",
       "       [9.99872208e-01],\n",
       "       [9.99831200e-01],\n",
       "       [8.95666957e-01],\n",
       "       [9.99524713e-01],\n",
       "       [6.13681797e-04],\n",
       "       [9.98430312e-01],\n",
       "       [9.99694228e-01],\n",
       "       [9.99818861e-01],\n",
       "       [1.55832793e-04],\n",
       "       [9.99708593e-01],\n",
       "       [9.99854207e-01],\n",
       "       [1.61820371e-03],\n",
       "       [8.25327879e-04],\n",
       "       [1.74901918e-01],\n",
       "       [8.72757460e-04],\n",
       "       [9.99322534e-01],\n",
       "       [9.99868035e-01],\n",
       "       [9.99869943e-01],\n",
       "       [2.69804877e-04],\n",
       "       [1.94014734e-04],\n",
       "       [1.78918242e-04],\n",
       "       [1.00908972e-01],\n",
       "       [2.72820733e-04],\n",
       "       [7.24718794e-02],\n",
       "       [9.99693871e-01],\n",
       "       [1.44729012e-04],\n",
       "       [7.88889766e-01],\n",
       "       [2.62464327e-03],\n",
       "       [2.40350468e-03],\n",
       "       [9.99839425e-01],\n",
       "       [3.85917345e-04],\n",
       "       [9.99715984e-01],\n",
       "       [3.03451414e-03],\n",
       "       [7.82872448e-05],\n",
       "       [5.17410284e-04],\n",
       "       [9.99877095e-01],\n",
       "       [9.99664545e-01],\n",
       "       [2.58409680e-04],\n",
       "       [6.46952773e-03],\n",
       "       [9.98967886e-01],\n",
       "       [2.09468053e-04],\n",
       "       [9.99829173e-01],\n",
       "       [4.89922240e-04],\n",
       "       [9.96538162e-01],\n",
       "       [1.66174248e-01],\n",
       "       [1.61728283e-04],\n",
       "       [8.54167223e-01],\n",
       "       [9.30095389e-02],\n",
       "       [7.48749764e-04],\n",
       "       [9.99917626e-01],\n",
       "       [9.64235216e-02],\n",
       "       [2.14092061e-02],\n",
       "       [9.99916315e-01],\n",
       "       [5.19106805e-01],\n",
       "       [6.43254668e-02],\n",
       "       [1.28443717e-04],\n",
       "       [9.10372853e-01],\n",
       "       [9.99545395e-01],\n",
       "       [9.99896407e-01],\n",
       "       [1.25136580e-02],\n",
       "       [7.30739115e-03],\n",
       "       [9.99874473e-01],\n",
       "       [3.99660945e-01],\n",
       "       [9.99874234e-01],\n",
       "       [3.99660945e-01],\n",
       "       [9.99870777e-01],\n",
       "       [1.48125764e-04],\n",
       "       [2.57507293e-03],\n",
       "       [1.29531865e-04],\n",
       "       [9.99881744e-01],\n",
       "       [2.83813407e-03],\n",
       "       [1.24821570e-02],\n",
       "       [1.63370889e-04],\n",
       "       [5.14828484e-04],\n",
       "       [3.85822816e-04],\n",
       "       [8.81463406e-04],\n",
       "       [5.27852848e-02],\n",
       "       [9.40257800e-04],\n",
       "       [2.47124862e-03],\n",
       "       [9.99871492e-01],\n",
       "       [7.26143480e-04],\n",
       "       [1.01147825e-02],\n",
       "       [1.32181384e-02],\n",
       "       [2.18268469e-04],\n",
       "       [4.98627633e-01],\n",
       "       [9.99818027e-01],\n",
       "       [5.39706962e-04],\n",
       "       [2.58474727e-04],\n",
       "       [4.58738999e-03],\n",
       "       [9.99795258e-01],\n",
       "       [9.99842763e-01],\n",
       "       [1.51079381e-04],\n",
       "       [6.52566145e-04],\n",
       "       [1.84565346e-04],\n",
       "       [6.09600575e-05],\n",
       "       [9.99872327e-01],\n",
       "       [2.18268469e-04],\n",
       "       [4.18568123e-03],\n",
       "       [9.98713136e-01],\n",
       "       [9.99853492e-01],\n",
       "       [9.99859214e-01],\n",
       "       [9.99922514e-01],\n",
       "       [9.99960542e-01],\n",
       "       [9.88703803e-04],\n",
       "       [1.40430566e-04],\n",
       "       [1.52926430e-01],\n",
       "       [9.99577940e-01],\n",
       "       [9.99869704e-01],\n",
       "       [9.99566615e-01],\n",
       "       [2.98881700e-04],\n",
       "       [9.99795973e-01],\n",
       "       [5.62456429e-01],\n",
       "       [5.76976308e-05],\n",
       "       [2.08081590e-04],\n",
       "       [8.53712764e-03],\n",
       "       [3.22400848e-03],\n",
       "       [1.10468238e-04],\n",
       "       [8.39721877e-04],\n",
       "       [9.99192774e-01],\n",
       "       [9.99899149e-01],\n",
       "       [1.58065159e-04],\n",
       "       [9.99867439e-01],\n",
       "       [9.99614239e-01],\n",
       "       [9.83491167e-03],\n",
       "       [6.91268127e-04],\n",
       "       [9.99317884e-01],\n",
       "       [7.06840158e-01],\n",
       "       [1.87638000e-01],\n",
       "       [1.31464825e-04]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred = model_3.predict(val_sentences)\n",
    "model_3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7598425196850394,\n",
       " 'precision': 0.7522935779816514,\n",
       " 'recall': 0.7068965517241379,\n",
       " 'f1-score': 0.7590768821741806}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_results = evaluation(val_labels,\n",
    "                             tf.squeeze(tf.round(model_3_pred)))\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_3_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional model\n",
    "inputs = layers.Input(shape= (1, ), dtype= 'string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences= True), name= 'birectional_LSTM_layer_1')(x)\n",
    "x = layers.Bidirectional(layers.GRU(64), name= 'bidirectional_LSTM_layer_2')(x)\n",
    "# x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs, outputs, name= 'model_4_bidirectional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_bidirectional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " birectional_LSTM_layer_1 (  (None, 15, 128)           98816     \n",
      " Bidirectional)                                                  \n",
      "                                                                 \n",
      " bidirectional_LSTM_layer_2  (None, 128)               74496     \n",
      "  (Bidirectional)                                                \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1453441 (5.54 MB)\n",
      "Trainable params: 1453441 (5.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_bidirectional/20240312-191442\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 16s 51ms/step - loss: 0.0737 - accuracy: 0.9766 - val_loss: 1.2118 - val_accuracy: 0.7612\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 7s 33ms/step - loss: 0.0368 - accuracy: 0.9813 - val_loss: 1.6129 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 7s 32ms/step - loss: 0.0414 - accuracy: 0.9815 - val_loss: 1.6554 - val_accuracy: 0.7546\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 7s 30ms/step - loss: 0.0358 - accuracy: 0.9816 - val_loss: 1.4411 - val_accuracy: 0.7677\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.0334 - accuracy: 0.9818 - val_loss: 1.7528 - val_accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(x= train_sentences,\n",
    "            y= train_labels,\n",
    "            epochs= 5,\n",
    "            validation_data= (val_sentences, val_labels),\n",
    "            callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_4_bidirectional')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "model_4_pred = model_4.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7664041994750657,\n",
       " 'precision': 0.7814569536423841,\n",
       " 'recall': 0.6781609195402298,\n",
       " 'f1-score': 0.7642857598265804}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_results = evaluation(val_labels,\n",
    "                             tf.squeeze(tf.round(model_4_pred)))\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_4_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for Text and other types of sequences\n",
    "\n",
    "We've used CNNs for image dataset which are 2D, however, our text data is 1D.\n",
    "\n",
    "Previously for image we used Conv2D, but for text data we will use Conv1D\n",
    "\n",
    "Architecture:\n",
    "```\n",
    "Input -> Tokenization -> Embedding -> CNN and Dense layers -> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "inputs = layers.Input(shape= (1, ), dtype= 'string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(64, 3, name= 'cnn_1d_layer_1')(x)\n",
    "x = layers.MaxPool1D(name= 'max_pool_layer_1')(x)\n",
    "x = layers.Conv1D(32, 3, name= 'cnn_1d_layer_2')(x)\n",
    "x = layers.MaxPool1D(name= 'max_pool_layer_2')(x)\n",
    "x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs, outputs, name= 'model_5_cnn_1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_cnn_1d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " cnn_1d_layer_1 (Conv1D)     (None, 13, 64)            24640     \n",
      "                                                                 \n",
      " max_pool_layer_1 (MaxPooli  (None, 6, 64)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " cnn_1d_layer_2 (Conv1D)     (None, 4, 32)             6176      \n",
      "                                                                 \n",
      " max_pool_layer_2 (MaxPooli  (None, 2, 32)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glo  (None, 32)                0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1310849 (5.00 MB)\n",
      "Trainable params: 1310849 (5.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_5_cnn_1d/20240312-192748\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 7s 26ms/step - loss: 0.1346 - accuracy: 0.9526 - val_loss: 0.9021 - val_accuracy: 0.7743\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.0730 - accuracy: 0.9742 - val_loss: 1.0374 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0604 - accuracy: 0.9766 - val_loss: 1.1580 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.0584 - accuracy: 0.9752 - val_loss: 1.2200 - val_accuracy: 0.7612\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.0532 - accuracy: 0.9787 - val_loss: 1.3146 - val_accuracy: 0.7533\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(x = train_sentences,\n",
    "                              y= train_labels,\n",
    "                              epochs= 5,\n",
    "                              validation_data= (val_sentences, val_labels),\n",
    "                              callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_5_cnn_1d')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "model_5_pred = model_5.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_results = evaluation(val_labels,\n",
    "                             tf.squeeze(tf.round(model_5_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7532808398950132,\n",
       " 'precision': 0.7469135802469136,\n",
       " 'recall': 0.6954022988505747,\n",
       " 'f1-score': 0.7523500583553816}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_5_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load('https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.07687581  0.03288896  0.02179662 -0.04832654 -0.02299898 -0.00182735\n",
      " -0.03993054  0.08256393  0.01381563  0.03039743  0.01625713  0.02431568\n",
      "  0.01514284  0.04843347  0.01244256  0.01343128  0.0023421  -0.02713761\n",
      " -0.00362572  0.00168669  0.01138663  0.0144208  -0.01719462 -0.03496176\n",
      " -0.07877127  0.04676209  0.03929747  0.02202034  0.01709515  0.0230419\n",
      "  0.0915276  -0.03294362 -0.00317207 -0.04850543 -0.09554192  0.04421337\n",
      " -0.02980342  0.01039704 -0.0229306   0.06235658 -0.01796251 -0.02358888\n",
      " -0.02192312 -0.04135567  0.01966412  0.01150151 -0.0491185  -0.03221893\n",
      " -0.02000231 -0.04916013], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed_samples = embed([\"When you complete with the downloading of the model then you will look into the encodings of this sentence\"])\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras Layer using the USE pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\", \n",
    "                                        input_shape= [],\n",
    "                                        dtype= 'string',\n",
    "                                        trainable= False,\n",
    "                                        name= 'use_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " use_layer (KerasLayer)      (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_layer_1 (Dense)       (None, 64)                32832     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model using Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64, activation='relu', name= 'dense_layer_1'),\n",
    "    layers.Dense(1, activation= 'sigmoid', name= 'output_layer')\n",
    "], name= 'model_6_USE')\n",
    "\n",
    "model_6.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_6_USE/20240312-232632\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 45s 196ms/step - loss: 0.4987 - accuracy: 0.7805 - val_loss: 0.4528 - val_accuracy: 0.8005\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 24s 114ms/step - loss: 0.4183 - accuracy: 0.8119 - val_loss: 0.4526 - val_accuracy: 0.8058\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.4073 - accuracy: 0.8210 - val_loss: 0.4448 - val_accuracy: 0.8071\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 19s 89ms/step - loss: 0.4025 - accuracy: 0.8238 - val_loss: 0.4411 - val_accuracy: 0.8097\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 18s 85ms/step - loss: 0.3988 - accuracy: 0.8251 - val_loss: 0.4399 - val_accuracy: 0.8176\n"
     ]
    }
   ],
   "source": [
    "model_6_history = model_6.fit(x= train_sentences,\n",
    "            y= train_labels,\n",
    "            epochs= 5,\n",
    "            validation_data= (val_sentences, val_labels),\n",
    "            callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_6_USE')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 8s 311ms/step\n"
     ]
    }
   ],
   "source": [
    "model_6_pred = model_6.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8175853018372703,\n",
       " 'precision': 0.8317460317460318,\n",
       " 'recall': 0.7528735632183908,\n",
       " 'f1-score': 0.8165413215854364}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_results = evaluation(val_labels,\n",
    "                             tf.squeeze(tf.round(model_6_pred)))\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_6_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: TF hub pretrained USE but with 10% of training data\n",
    "\n",
    "Transfer learning helps alot when you don't have a large dataset.\n",
    "\n",
    "Let's replicate `model_6` with only 10% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(761, 761)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create subsets of 10% of the training data\n",
    "train_10_percent = train_df_shuffled[['text', 'target']].sample(frac= 0.1, random_state= 42)\n",
    "train_sentences_10_percent = train_10_percent['text'].to_list()\n",
    "train_labels_10_percent = train_10_percent['target'].to_list()\n",
    "len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    413\n",
       "1    348\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of targets in our subsets of data\n",
    "train_10_percent['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_shuffled['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_sentences_10_percent, train_labels_10_percent, test_size= 0.1, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " use_layer (KerasLayer)      (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_layer_1 (Dense)       (None, 64)                32832     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256830721 (979.73 MB)\n",
      "Trainable params: 32897 (128.50 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7 = tf.keras.models.clone_model(model_6)\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_7_10_percent/20240312-234523\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 14s 484ms/step - loss: 0.4176 - accuracy: 0.8129 - val_loss: 0.4413 - val_accuracy: 0.7922\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 0.3969 - accuracy: 0.8319 - val_loss: 0.4420 - val_accuracy: 0.7922\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 4s 200ms/step - loss: 0.3817 - accuracy: 0.8421 - val_loss: 0.4385 - val_accuracy: 0.7792\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 4s 172ms/step - loss: 0.3690 - accuracy: 0.8450 - val_loss: 0.4410 - val_accuracy: 0.7792\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.3601 - accuracy: 0.8523 - val_loss: 0.4395 - val_accuracy: 0.7662\n"
     ]
    }
   ],
   "source": [
    "model_7_history = model_7.fit(X_train,\n",
    "            y_train,\n",
    "            epochs= 5,\n",
    "            validation_data= (X_test, y_test),\n",
    "            callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_7_10_percent')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 509ms/step\n"
     ]
    }
   ],
   "source": [
    "model_7_pred = model_7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7662337662337663,\n",
       " 'precision': 0.6756756756756757,\n",
       " 'recall': 0.8064516129032258,\n",
       " 'f1-score': 0.7683923463677226}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_results = evaluation(y_test,\n",
    "                             tf.squeeze(tf.round(model_7_pred)))\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_7_results)) > np.array(list(baseline_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
