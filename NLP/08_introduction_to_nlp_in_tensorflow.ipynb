{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundatmentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language\n",
    "\n",
    "Another common term for NLP problem is seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading helper function inside the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going ti be using is Kaggle's introduction to NLP dataset. A classification problem\n",
    "\n",
    "[Competition Link](https://www.kaggle.com/competitions/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 11:10:55--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4009:805::201b, 2404:6800:4009:806::201b, 2404:6800:4009:809::201b, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4009:805::201b|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: 'nlp_getting_started.zip.1'\n",
      "\n",
      "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-03-12 11:10:55 (4.80 MB/s) - 'nlp_getting_started.zip.1' saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "\n",
    "unzip_data('nlp_getting_started.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read thed data\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac= 1, random_state= 42)\n",
    "\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class\n",
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How may samples\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Not a disaster\n",
      "Text: Mike Magner Discusses A Trust Betrayed: http://t.co/GETBjip5Rh via @YouTube #military #veterans #environment\n",
      "----\n",
      "\n",
      "Target: Not a disaster\n",
      "Text: Beware of your temper and a loose tongue! These two dangerous weapons combined can lead a person to the Hellfire #islam!\n",
      "----\n",
      "\n",
      "Target: Disaster\n",
      "Text: Flood: Two people dead 60 houses destroyed in Kaduna: Two people have been reportedly killed and 60 houses ut... http://t.co/BDsgF1CfaX\n",
      "----\n",
      "\n",
      "Target: Disaster\n",
      "Text: MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ... - ABC Onlin... http://t.co/N3lNdJKYo3 G #Malaysia #News\n",
      "----\n",
      "\n",
      "Target: Not a disaster\n",
      "Text: Whenever I have a meltdown and need someone @Becca_Caitlyn99 is always like 'leaving in 5' and I don't know how I got so lucky #blessed\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualise some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "\n",
    "for row in train_df_shuffled[['text', 'target']][random_index: random_index + 5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {'Disaster' if target == 1 else 'Not a disaster'}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"----\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split() to split the trianing data into train and validation dataset\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(), train_df_shuffled['target'].to_numpy(), test_size= 0.1, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 sentences\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to nuumbers.\n",
    "\n",
    "There are a few ways to do this:\n",
    "* Tokenization\n",
    "* Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Use defalut TextVectorization parameter\n",
    "text_vectorizer = TextVectorization(max_tokens= None, # how many words in vocab\n",
    "                                    standardize= 'lower_and_strip_punctuation',\n",
    "                                    split= 'whitespace',\n",
    "                                    ngrams= None, # creates a group of words\n",
    "                                    output_mode= 'int', # in which format the output should be\n",
    "                                    output_sequence_length= None, # how long deos the output sequence should be of\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokens in the training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences]) // len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocab\n",
    "max_length = 15 # max length our sequences will be\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens= max_vocab_length,\n",
    "                                    output_mode= 'int',\n",
    "                                    output_sequence_length= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorization to training set\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentences = \"There's a flood in my street!\"\n",
    "\n",
    "text_vectorizer([sample_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: -??-\n",
      "; kitana\n",
      "? her fatalities slay me\n",
      "ÛÓkody? (Vine by @KOMBATFANS33) https://t.co/uMajwSNLUF\n",
      "Vectorized sentence: [[   1   81  301 8417   31 6431 1927   18    1    1    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# Choose a random sentences from the training sentences\n",
    "random_sentences = random.choice(train_sentences)\n",
    "print(f\"Original text: {random_sentences}\")\n",
    "print(f\"Vectorized sentence: {text_vectorizer([random_sentences])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo words in vocab: 10000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary() # retrieves all of the unique words\n",
    "\n",
    "tope_5_words = words_in_vocab[:5] # gets the most common words\n",
    "bottom_5_words = words_in_vocab[-5:] # gets the least common words\n",
    "\n",
    "print(f\"Number fo words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {tope_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding layer\n",
    "\n",
    "We're going to use Tensorflow Embedding layer.\n",
    "\n",
    "\n",
    "The parameters we care most about our embedding layers:\n",
    "* `input_dim`\n",
    "* `output_dim`\n",
    "* `input_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x2942dccd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim= max_vocab_length, # set input shape\n",
    "                             output_dim= 128,\n",
    "                             input_length= max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.043427  , -0.02271678, -0.00248923, ..., -0.03483094,\n",
       "         -0.01346254, -0.03058391],\n",
       "        [ 0.03267677,  0.04190341, -0.04285845, ...,  0.0135429 ,\n",
       "         -0.02445282, -0.03918562],\n",
       "        [ 0.01630927, -0.03321409, -0.00125452, ...,  0.02345994,\n",
       "          0.02224901,  0.00741848],\n",
       "        ...,\n",
       "        [-0.02616034,  0.00962585,  0.0474998 , ...,  0.03219023,\n",
       "          0.0456875 ,  0.04080323],\n",
       "        [-0.01575265,  0.04413174,  0.01501777, ..., -0.04161366,\n",
       "         -0.03297486,  0.00340591],\n",
       "        [-0.01612275,  0.04820825,  0.0210881 , ...,  0.02680815,\n",
       "          0.03131999, -0.00614514]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a ranomd sentence from the training set\n",
    "random_sentences = random.choice(train_sentences)\n",
    "\n",
    "print(f\"Original sentences: {random_sentences}\")\n",
    "\n",
    "# Embed the random sentences\n",
    "sample_embde = embedding(text_vectorizer([random_sentences]))\n",
    "sample_embde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.043427  , -0.02271678, -0.00248923, -0.04603863,  0.00509753,\n",
       "         0.01818783, -0.00246652,  0.01118038,  0.00157026,  0.04659058,\n",
       "        -0.01123718, -0.00972148,  0.04788823,  0.01407531, -0.02020611,\n",
       "        -0.03455592,  0.04628216, -0.04112779,  0.04135301, -0.02473733,\n",
       "         0.03040474,  0.02041018,  0.00029361, -0.04249674,  0.0377947 ,\n",
       "        -0.00522221,  0.03419198, -0.01971959,  0.02616808,  0.03351443,\n",
       "        -0.0458042 , -0.02779064,  0.04195997, -0.02212015, -0.03683157,\n",
       "         0.00789969,  0.02253792, -0.04799919, -0.03994947, -0.04821298,\n",
       "        -0.04259375,  0.02873099,  0.03656069, -0.04618149, -0.04235861,\n",
       "        -0.03062811,  0.02972574, -0.01694834,  0.03712305,  0.03289719,\n",
       "         0.04936178, -0.01675064, -0.04377675,  0.02584152,  0.04081562,\n",
       "        -0.00031313, -0.00864778, -0.00172927,  0.02708354, -0.0185649 ,\n",
       "        -0.04001318,  0.00172317,  0.01937211,  0.00556774,  0.00397693,\n",
       "        -0.03631838, -0.04462849, -0.02080899,  0.04514698, -0.01922944,\n",
       "        -0.02374872, -0.0468714 , -0.01077871, -0.03157955, -0.04873264,\n",
       "        -0.00965784, -0.02315221,  0.03655436, -0.04466926,  0.01839526,\n",
       "        -0.04151868, -0.01684328,  0.00872016, -0.03104689,  0.02647151,\n",
       "        -0.0468613 ,  0.0338442 , -0.0098135 , -0.01127733,  0.04500835,\n",
       "         0.03906084, -0.04139799, -0.04829485,  0.04400598, -0.0224173 ,\n",
       "         0.01750404, -0.00151638,  0.01769127,  0.02168074, -0.02500168,\n",
       "        -0.04846506,  0.01938829, -0.01911032, -0.04346162, -0.04515893,\n",
       "        -0.02620918, -0.048883  ,  0.04580424,  0.02927433, -0.02087658,\n",
       "         0.0016773 , -0.0059906 , -0.02303615, -0.03287779,  0.02705491,\n",
       "        -0.03646349,  0.04304031,  0.02123678, -0.00470868, -0.02931422,\n",
       "         0.0488063 , -0.01934584,  0.00137708, -0.0493651 , -0.0026213 ,\n",
       "        -0.03483094, -0.01346254, -0.03058391], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 't')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embde[0][0], sample_embde[0][0].shape, random_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling a text dataset\n",
    "\n",
    "Experiments:\n",
    "\n",
    "* Model 0: Naive Bayes\n",
    "* Model 1: Feed-forward neural network\n",
    "* Model 2: LSTM model\n",
    "* Model 3: GRU model\n",
    "* Model 4: Bidirectional LSTM model\n",
    "* Model 5: 1D Convolutional Neural Network\n",
    "* Model 6: Tensorflow Hub pretrained fetaure extractor\n",
    "* Model 7: Same as Model 6 but with 10% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "> **Note**: It is common practice to use non-DL algorithm as a baseline because of their speed and then alter using DL to see if you can improve upon them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenizer and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert the text data into numerical form using tf-idf technique\n",
    "    (\"clf\", MultinomialNB()) # classifier for the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves: {baseline_score * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation function to evaluate the model\n",
    "# A function which returns a dictionary of accuracy, precision, recall and f1-score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluation(y_true, y_pred):\n",
    "    '''\n",
    "    This function gives the evaluation of the classification model in a dictionary form (Accuracy, Precision, Recall and F1-score)\n",
    "\n",
    "    Parameters:\n",
    "        y_true: The true labels of the evaluation data\n",
    "        y_pred: The predicted labels of the evaluation data\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys: `accuracy`, `precision`, `recall`, `f1-score`\n",
    "    '''\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1score = f1_score(y_true, y_pred, average= 'weighted') # we are using weighted  average because the dataset is slightly imbalanced\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = evaluation(y_true= val_labels, \n",
    "           y_pred= baseline_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save tensorboard callback logs\n",
    "SAVED_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functiobnal API\n",
    "\n",
    "inputs = layers.Input(shape= (1, ), dtype= tf.string)\n",
    "x = text_vectorizer(inputs) # convert text to numbers\n",
    "x = embedding(x) # convert numbers to matrix\n",
    "x = layers.GlobalAveragePooling1D(name= 'global_avg_pool_layer')(x)\n",
    "outputs = layers.Dense(1, activation= 'sigmoid', name= 'output_layer')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs, name= 'model_1_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_avg_pool_layer (Glo  (None, 128)               0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_1.compile(loss= 'binary_crossentropy',\n",
    "                optimizer= tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20240312-130658\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 16ms/step - loss: 0.6111 - accuracy: 0.6871 - val_loss: 0.5362 - val_accuracy: 0.7585\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.4403 - accuracy: 0.8183 - val_loss: 0.4701 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.3456 - accuracy: 0.8610 - val_loss: 0.4577 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2838 - accuracy: 0.8920 - val_loss: 0.4646 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.2376 - accuracy: 0.9099 - val_loss: 0.4869 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(x = train_sentences,\n",
    "                              y= train_labels,\n",
    "                              epochs= 5,\n",
    "                              validation_data= (val_sentences, val_labels),\n",
    "                              callbacks= [create_tensorboard_callback(SAVED_DIR, 'model_1_dense')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4869 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4868801534175873, 0.7887139320373535]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = model_1.predict(val_sentences)\n",
    "model_1_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = evaluation(y_true= val_labels,\n",
    "           y_pred= tf.round(tf.squeeze(model_1_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
